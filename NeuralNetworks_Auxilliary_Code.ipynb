{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "NeuralNetworks_Auxilliary_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI4sFar7LXGv"
      },
      "source": [
        "# CZ4041 Machine Learning Project\n",
        "## Kaggle Competition: New York City Taxi Trip Duration Prediction\n",
        "\n",
        "## Source Code by Group 21\n",
        "---\n",
        "\n",
        "In this notebook, we have created a Neural Network model for NYC Taxi Trip Duration Prediction using TensorFlow 2 / Keras.\n",
        "\n",
        "This notebook is provided separately to allow the notebook to be run independently, and to be run in parallel in multiple sessions (i.e. separate Google Colaboratory sessions for increased parallelization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx2wYpEXbpZy"
      },
      "source": [
        "# Essential libraries\n",
        "import pandas as pd  # Pandas for using dataframes and reading/writing CSVs \n",
        "import numpy as np   # Numpy for vector operations and basic math\n",
        "import tensorflow as tf # For Deep Learning - to work with neural networks\n",
        "\n",
        "# To split the original train dataset into train and validation sets\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Additional libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WN01AjgOVMs"
      },
      "source": [
        "### Read the complete train and test datasets after saving them in the main notebook (EDA_FeatureEngineering_XGBoost.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaEKZ_1X3SF8"
      },
      "source": [
        "train = pd.read_csv('train_complete.csv')\n",
        "test = pd.read_csv('test_complete.csv')\n",
        "print(f'Shape of Train Set: {train.shape}\\nShape of Test Set: {test.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scYFIZHAvo97"
      },
      "source": [
        "# To check whether the train data has the correct columns/features.\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPcnIjwUviqs"
      },
      "source": [
        "# List of important/selected features\n",
        "feature_names = ['vendor_id','passenger_count', 'pickup_longitude', 'pickup_latitude',\n",
        "    'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag', 'pickup_pca0',\n",
        "    'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1', 'distance_haversine', \n",
        "    'distance_dummy_manhattan', 'direction', 'pca_manhattan', 'center_latitude',\n",
        "    'center_longitude', 'pickup_weekday', 'pickup_month', 'pickup_hour_weekofyear',\n",
        "    'pickup_hour', 'pickup_minute', 'pickup_dt', 'pickup_week_hour',\n",
        "    'pickup_day', 'pickup_week', 'pickup_minute_of_the_day', 'pickup_dayofyear',\n",
        "    'pickup_am', 'night_trip', 'rush_hour', 'weekday',\n",
        "    'pickup_is_weekend', 'pickup_holiday', 'pickup_near_holiday', 'pickup_businessday',\n",
        "    'pickup_cluster', 'dropoff_cluster', 'avg_speed_h_gby_pickup_hour', 'avg_speed_m_gby_pickup_hour',\n",
        "    'log_trip_duration_gby_pickup_hour', 'avg_speed_h_gby_pickup_date', 'avg_speed_m_gby_pickup_date',\n",
        "    'log_trip_duration_gby_pickup_date', 'avg_speed_h_gby_pickup_dt_bin', 'avg_speed_m_gby_pickup_dt_bin',\n",
        "    'log_trip_duration_gby_pickup_dt_bin', 'avg_speed_h_gby_pickup_week_hour', 'avg_speed_m_gby_pickup_week_hour',\n",
        "    'log_trip_duration_gby_pickup_week_hour', 'avg_speed_h_gby_pickup_cluster', 'avg_speed_m_gby_pickup_cluster',\n",
        "    'log_trip_duration_gby_pickup_cluster','avg_speed_h_gby_dropoff_cluster','avg_speed_m_gby_dropoff_cluster',\n",
        "    'log_trip_duration_gby_dropoff_cluster','avg_speed_h_center_lat_bin_center_long_bin',\n",
        "    'cnt_center_lat_bin_center_long_bin', 'avg_speed_h_pickup_hour_center_lat_bin_center_long_bin',\n",
        "    'cnt_pickup_hour_center_lat_bin_center_long_bin', 'avg_speed_h_pickup_hour_pickup_cluster',\n",
        "    'cnt_pickup_hour_pickup_cluster', 'avg_speed_h_pickup_hour_dropoff_cluster',\n",
        "    'cnt_pickup_hour_dropoff_cluster', 'avg_speed_h_pickup_cluster_dropoff_cluster',\n",
        "    'cnt_pickup_cluster_dropoff_cluster', 'count_60min', 'dropoff_cluster_count','pickup_cluster_count',\n",
        "    'total_distance', 'total_travel_time', 'number_of_steps'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLrTgje359Ux"
      },
      "source": [
        "print('We have %i features.' % len(feature_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St9q2LzEe_Eh"
      },
      "source": [
        "# Main Neural Networks Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2ys4T1RqKZ"
      },
      "source": [
        "## Setting Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnFzrg8fbpaB"
      },
      "source": [
        "# Please set the hyperparameters to be tested here.\n",
        "# Note: learning rate is to be set manually in the next cell\n",
        "\n",
        "NUM_NEURONS = 400\n",
        "NUM_LAYERS = 40\n",
        "\n",
        "OPTIMIZER = \"adam\" # Choose from between: sgd, momentum, rmsprop, and adam\n",
        "L2_REGULARIZATION = 5e-5\n",
        "DROP_RATE = 0 # For Dropout\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF0bKI0jQpAQ"
      },
      "source": [
        "# Code to choose optimizer\n",
        "if OPTIMIZER == \"adam\":\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)\n",
        "if OPTIMIZER == \"sgd\":\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0, nesterov=False)\n",
        "if OPTIMIZER == \"momentum\":\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False) # Can vary momentum if needed\n",
        "if OPTIMIZER == \"rmsprop\":\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3, rho=0.9, momentum=0.0, epsilon=1e-7)\n",
        "else:\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybaCIoCRuPI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NKerpFrae8r"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "train_ = train[feature_names].values\n",
        "train_normalized = sc.fit_transform(train_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIqQQlk5Rv4m"
      },
      "source": [
        "## Splitting Training Set into Train and Validation sets\n",
        "\n",
        "First, creating the \"y\" vector we stores the log of target trip duration values + 1.\n",
        "\n",
        "Also zipping the train and validation sets into TensorFlow datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N_o7Lk_9hnm"
      },
      "source": [
        "y = np.log(train['trip_duration'].values + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUF7o2kObpaB"
      },
      "source": [
        "# Training\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_normalized, y, test_size=0.2, random_state=7) # Random state 7 to improve consistency over multiple runs\n",
        "X_train = np.nan_to_num(X_train, copy=False, nan=0)\n",
        "X_val = np.nan_to_num(X_val, copy=False, nan=0)\n",
        "\n",
        "train_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(X_train.tolist()), tf.data.Dataset.from_tensor_slices(y_train.tolist())))\n",
        "val_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(X_val.tolist()), tf.data.Dataset.from_tensor_slices(y_val.tolist())))\n",
        "\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-XxV0W8SJt-"
      },
      "source": [
        "## Batching the train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnao9C5EbpaC"
      },
      "source": [
        "batched_train_ds = train_dataset.batch(BATCH_SIZE)\n",
        "batched_val_ds = val_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vCH7W9pSLpK"
      },
      "source": [
        "## Building the model using TensorFlow's Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCCmHr7vbpaC"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "for i in range(NUM_LAYERS):\n",
        "    model.add(tf.keras.layers.Dense(NUM_NEURONS, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARIZATION)))\n",
        "    \n",
        "    # Uncomment this for Dropout Regularization\n",
        "    # if (i + 1) % (range(NUM_LAYERS) / 5) == 0:\n",
        "    #     model.add(tf.keras.layers.Dropout(rate=DROP_RATE)) \n",
        "\n",
        "model.add(tf.keras.layers.Dense(1, activation=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3g0d1QYTFJd"
      },
      "source": [
        "# Training the Neural Network Model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPq9CpI1bpaC"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        ")\n",
        "history = model.fit(\n",
        "    x=batched_train_ds, validation_data=batched_val_ds,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2DXW43um3qB"
      },
      "source": [
        "# Evaluate on Validation Set\n",
        "model.evaluate(x=batched_val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaB6q1IUUFIr"
      },
      "source": [
        "# Producing the Final Predictions on the Original Kaggle NYC Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1JGcgPdd1u"
      },
      "source": [
        "# Applying feature scaling by transforming based on the Scaler used for the train set\n",
        "test_normalized = sc.transform(test[feature_names].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aunOvRuIbpaC"
      },
      "source": [
        "# Preparing the test data for use with the neural network model\n",
        "final_test = test_normalized\n",
        "final_test = np.nan_to_num(final_test, copy=False, nan=0)\n",
        "final_test_ds = tf.data.Dataset.from_tensor_slices(final_test.tolist())\n",
        "batched_final_test = final_test_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PJlGtXVbpaC"
      },
      "source": [
        "# Producing the final Kaggle submission CSV for neural networks!\n",
        "ytest = model.predict(batched_final_test)\n",
        "print('Test shape OK.') if test.shape[0] == ytest.shape[0] else print('Oops')\n",
        "test['trip_duration'] = np.exp(ytest) - 1\n",
        "test['id'] = test_original['id']\n",
        "test[['id', 'trip_duration']].to_csv('submission_neural_network.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhHcj7oOUnwk"
      },
      "source": [
        "## That's the end of the code for Neural Networks and this project as a whole! Thanks for reading!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1XFt94bA6J8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}